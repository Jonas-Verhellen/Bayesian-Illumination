# Graph-Based Bayesian Illumination (GB-BI)

<p align="center">
  <a href="[https://github.com/Jonas-Verhellen/Bayesian-Illumination]">
    <img src="imgs/GB-BI-banner-2-compressed.png" width="100%" />
  </a>
</p>

[![Project Status: Active â€“ The project has reached a stable, usable state and is being actively developed.](https://www.repostatus.org/badges/latest/active.svg)](https://www.repostatus.org/#active)
[![License: MIT](https://img.shields.io/github/license/daq-tools/wireviz-web)](https://opensource.org/license/agpl-v3)

**Graph-Based Bayesian Illumination (GB-BI)** is an open-source software library that aims to make state-of-the-art, quality-diversity optimisation techniques infused with Bayesian optimisation easily accessible to scientific experts in medicinal chemistry and cheminformatics. We provide a modular codebase, novel benchmarks, and extensive documentation.

[Overview](#overview) | [Getting Started](#getting-started) | [Documentation](https://jonas-verhellen.github.io/Bayesian-Illumination/index.html#)| [Paper](https://arxiv.org) | [Benchmarks](https://github.com/Jonas-Verhellen/Bayesian-Illumination)


## Overview

Despite a surge of deep learning papers focused on generative models for small molecules, it remains difficult for these models to out-compete more traditional, rule-based approaches such as genetic algorithms. While comparatively efficient, genetic algorithms have two main drawbacks: susceptibility to stagnation (due to diversity collapse in the evolutionary population) and the lack of exploitation of the information encoded in the fitness values of the different molecules generated by the algorithm throughout its optimisation procedure. **Graph-Based Bayesian Illumination** solves both of these issues by combining the stepping-stone properties of quality-diversity methods (to escape stagnation) with the sampling efficiency of Bayesian optimisation to create a novel generative model that vastly outperforms both deep learning models and genetic algorithms.

<p align="left">
  <a href="[https://github.com/Jonas-Verhellen/Bayesian-Illumination]">
    <img src="imgs/GB-BI-overview.jpg" width="100%" />
  </a>
</p>

Standard quality-diversity (QD) and Bayesian optimisation (BO) libraries do not cater to particularities of optimising small molecules. Therefore, GB-BI relies on two packages that provide these utilities specifically for small molecules.

* **GB-EPI:** a [quality-diversity software package](https://github.com/Jonas-Verhellen/Argenomic) for the optimisation of small molecules which enforces diversity in chemical space by creating niches based on physicochemical properties.
* **GAUCHE:** a [Gaussian process framework](https://github.com/leojklarner/gauche) for chemistry, providing more than 30 bespoke kernels for small molecules, proteins and chemical reactions.

For the mutatations, crossovers, structural filters and physicochemical descriptors, we rely on [RDKit](https://github.com/rdkit/rdkit). The structural filters are based on a set of [scripts](https://github.com/PatWalters/rd_filters) provided by Patrick Walters. The graph-based mutations and crossovers are inspired by Jan H. Jensen's graph-based genetic algorithm ([GB-GA](https://github.com/jensengroup/GB_GA)). Ideas regarding the application of acquisition functions and Bayesian optimsation to quality-diversity methods (and MAP-Elites specifically) were spurred on by the publication of the [BOP-Elites algorithm](https://arxiv.org/abs/2005.04320).

## Getting Started

The easiest way to use GB-BI is to clone this repository and install the included conda environment.

## Features and Settings

### Fitness Functions

GB-BI provides three classes of fitness functions out-of-the-box: fingerprint-based rediscovery, descriptor-based rediscovery, and SAS-modulated docking scores. These fitness functions can and have been used as benchmark tools to probe the effciency of generative models but also have direct practical applications. Additional fitness functions can easily be added to the codebase.

* **Fingerprint-Based Rediscovery:** A lightweight task focused on molecule rediscovery where the fitness of a molecule is the Tanimoto similarity to the target molecule, based on their respective extended-connectivity fingerprints. Implementation based on Gaucamol.
* **Descriptor-Based Rediscovery:**  An alternative molecule rediscovery task, with intermediate computational expense, where the fitness of a generated molecule is the conformer-aggregated similarity to the target molecule, based on their respective descriptors (USRCAT or Zernike).
* **SAS-Modulated Docking Scores:** A computationally intensive task, utilising docking methods which evaluate the theoretical affinity between a small molecule and a target protein. To avoid pure exploitation of the docking method, the scores are modulated by the synthetic accesibility of the small molecule.

### Molecular Representations

GB-BI supports several molecular representation that are based on bit vectors or strings. These representations are used for the surrogate models using the Tanimoto kernel from GAUCHE. The string-based representations are turned into a bag-of-characters before being used in the kernel. Note that several of these vectors representations are currently not natively supported by GAUCHE.

<table>
<thead>
  <tr>
    <th>Representation</th>
    <th>Description</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td rowspan="1">ECFP</td>
    <td> Extended-Connectivity Fingerprints (ECFP) are circular topological fingerprints that represent the presence of particular substructures. </td>
  </tr>
  <tr>
    <td rowspan="1">FCFP</td>
    <td> Functional-Class Fingerprints (FCFP) are circular topological fingerprints that represent the presence of particular pharmacophoric properties. </td>
  </tr>
  <tr>
    <td rowspan="1">RDFP</td>
    <td> RDKit-specific fingerprints (RDFP) are inspired by public descriptions of the Daylight fingerprints, but differ significantly in practical implementation. </td>
  </tr>
  <tr>
    <td rowspan="1">APFP</td>
    <td> Atom pair fingerprints (APFP) encodes all unique triplets of atomic number, number of heavy atom neighbours, aromaticity and chirality in a vector format. </td>
  </tr>
  <tr>
    <td rowspan="1">TTFP</td>
    <td> Topological torsion fingerprints (TTFP) encode the long-range relationships captured in atom pair fingerprints through information on the torsion angles of a molecule. </td>
  </tr>
  <tr>
    <td rowspan="1">SMILES</td>
    <td> The simplified molecular-input line-entry system (SMILES) is a widely used line notation for describing the structure of a small molecule in terms of short ASCII strings. </td>
  </tr>
  <tr>
    <td rowspan="1">SELFIES</td>
    <td> Self-referencing embedded strings (SELFIES) are an alternative line notation for the structure of a small molecule, designed to be used in arbitrary machine learning models. </td>
  </tr>
</tbody>
</table>

### Acquisition Functions

Acquisition functions are heuristics employed to evaluate the potential of candidate moelcules based on their predicted fitness value and the associated uncertainty of a surrogate fitness model (i.e. the Gaussian process). A large literature exists on the topic of acquisition functions and their design. GB-BI supports several of the most well-known and often used acquisition functions.

* **Posterior Mean:** The posterior mean is simply the fitness value as predicted by the surrogate fitness model.
* **Upper Confidence Bound (UCB):** The upper confidence bound balances exploration and exploitation based on a confidence boundary derived from the surrogate fitness model. <!--The upper confidence bound is defined as $\text{UCB}(x) = \mu(x) + \beta \sigma(x)$, where $\mu(\cdot)$ and $\sigma(\cdot)$ are respectively the posterior mean and variance of the surrogate fitness model, $x$ denotes the candidate solution, and  $\beta$ is a hyperparameter representing confidence in the surrogate model.-->
* **Expected Improvement (EI):** The expected improvement considers both the probability of improving on the current solutions and the magnitude of the predicted improvement. <!--The expected improvement is defined as $\text{EI}(x) = \sigma(x) \ h\left((\mu(x) - y)/\sigma(x)\right)$, where $x$ denotes the candidate solution, $\mu(\cdot)$ and $\sigma(\cdot)$ are respectively the posterior mean and variance of the surrogate fitness model, and $y$ is the best fitness function value observed so far. In the above equation, the helper function $h(\cdot)$ is defined as $h(z) = \phi(z) + z \Phi(z)$ where $\phi$ and $\Phi$ are respectively the probability density function and the cumulative density function of the Normal distribution.-->
* **Numerically Stable log(EI) (logEI):** A numerically stable variant of the logarithm of the expected improvement (logEI), which was was recently introduced to alleviate the vanishing gradient problems sometimes encountered in the classical version of EI. <!-- and is defined as $\text{logEI}(x) = \text{log}_h\left((\mu(x) - y)/\sigma(x)\right) + \text{log}(\sigma(x))$ in which the helper function $\text{log}_h(\cdot)$ is a numerical stable implementation of the composite function $\log (h)$.-->

## Contributions

If there are any specific fitness functions, molecular representations or acquisition functions that you would like to see included in GB-BI, please open an issue or submit a pull request. More information on how to adapt and extend GB-BI can be found in the documentation.
